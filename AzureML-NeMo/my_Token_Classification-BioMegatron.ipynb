{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "[NeMo W 2020-10-22 17:12:27 experimental:28] Module <class 'nemo.collections.nlp.modules.common.huggingface.auto.AutoModelEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-22 17:12:27 experimental:28] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to describe how to finetune BioMegatron - a [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) model pre-trained on large biomedical text corpus ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts and full-text commercial use collection) - on the [NCBI Disease Dataset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/) for Named Entity Recognition.\n",
    "\n",
    "The model size of Megatron-LM can be larger than BERT, up to multi-billion parameters, compared to 345 million parameters of BERT-large.\n",
    "There are some alternatives of BioMegatron, most notably [BioBERT](https://arxiv.org/abs/1901.08746). Compared to BioBERT BioMegatron is larger by model size and pre-trained on larger text corpus.\n",
    "\n",
    "A more general tutorial of using BERT-based models, including Megatron-LM, for downstream natural language processing tasks can be found [here](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/01_Pretrained_Language_Models_for_Downstream_Tasks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description\n",
    "**Named entity recognition (NER)**, also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text.\n",
    "\n",
    "For instance, **given sentences from medical abstracts, what diseases are mentioned?**<br>\n",
    "In this case, our data input is sentences from the abstracts, and our labels are the precise locations of the named disease entities.  Take a look at the information provided for the dataset.\n",
    "\n",
    "For more details and general examples on Named Entity Recognition, please refer to the [Token Classification and Named Entity Recognition tutorial notebook](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Token_Classification_Named_Entity_Recognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) is a set of 793 PubMed abstracts, annotated by 14 annotators. The annotations take the form of HTML-style tags inserted into the abstract text using the clearly defined rules.  The annotations identify named diseases, and can be used to fine-tune a language model to identify disease mentions in future abstracts, *whether those diseases were part of the original training set or not*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of what an annotated abstract from the corpus looks like:\n",
    "\n",
    "```html\n",
    "10021369\tIdentification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor .\tThe <category=\"Modifier\">adenomatous polyposis coli ( APC ) tumour</category>-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In <category=\"Modifier\">colon carcinoma</category> cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - <category=\"Modifier\">colon carcinoma</category> cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and <category=\"SpecificDisease\">cancer</category> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we see the following tags within the abstract:\n",
    "```html\n",
    "<category=\"Modifier\">adenomatous polyposis coli tumour</category>\n",
    "<category=\"Modifier\">adenomatous polyposis coli ( APC ) tumour</category>\n",
    "<category=\"Modifier\">colon carcinoma</category>\n",
    "<category=\"Modifier\">colon carcinoma</category>\n",
    "<category=\"SpecificDisease\">cancer</category>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we will consider any identified category (such as \"Modifier\", \"Specific Disease\", and a few others) to generally be a \"disease\".\n",
    "\n",
    "Let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanse.sh\t     mlruns\n",
      "containersetup.sh    my_GLUE_Benchmark.ipynb\n",
      "create_data.py\t     my_Token_Classification-BioMegatron.ipynb\n",
      "DATA_DIR\t     NeMo\n",
      "go-gluetask.sh\t     nemo_experiments\n",
      "go-NERNLP.sh\t     setup_data.sh\n",
      "launch-nemo-2009.sh  setup_jupyter_env.ipynb\n",
      "launch-nemo-glue.sh  wandb\n",
      "launch-nemo-ner.sh   WORK_DIR\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA_DIR\"\n",
    "# os.makedirs(DATA_DIR, exist_ok=True)\n",
    "# os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly, for example typing a command like the following in a code-cell:\n",
    "<pre><code>\n",
    "! head -1 $DATA_DIR/NCBI_corpus_testing.txt\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'DATA_DIR/NCBI_corpus_testing.txt' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! head -1 $DATA_DIR/NCBI_corpus_testing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two datasets derived from this corpus:  a text classification dataset and a named entity recognition (NER) dataset.  The text classification dataset labels the abstracts among three broad disease groupings.  We'll use this simple split to demonstrate the NLP text classification task.   The NER dataset labels individual words as diseases.  This dataset will be used for the NLP NER task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataset\n",
    "A pre-processed NCBI-disease dataset for NER can be found [here](https://github.com/spyysalo/ncbi-disease/tree/master/conll) or [here](https://github.com/dmis-lab/biobert#datasets).<br>\n",
    "We download the files under {DATA_DIR/NER} directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.6M\n",
      "-rwxrwxrwx 1 root root 196K Oct 22 16:36 dev.tsv\n",
      "-rwxrwxrwx 1 root root  63K Oct 22 16:38 labels_dev.txt\n",
      "-rwxrwxrwx 1 root root  65K Oct 22 16:38 labels_test.txt\n",
      "-rwxrwxrwx 1 root root 360K Oct 22 16:38 labels_train.txt\n",
      "-rwxrwxrwx 1 root root 162K Oct 22 16:36 NCBI_corpus_development.txt\n",
      "-rwxrwxrwx 1 root root 171K Oct 22 16:36 NCBI_corpus_testing.txt\n",
      "-rwxrwxrwx 1 root root 938K Oct 22 16:36 NCBI_corpus_training.txt\n",
      "-rwxrwxrwx 1 root root 360K Oct 22 16:36 NCBI_corpus.zip\n",
      "-rwxrwxrwx 1 root root 6.7K Oct 22 16:51 sample_labels_dev.txt\n",
      "-rwxrwxrwx 1 root root  15K Oct 22 16:51 sample_text_dev.txt\n",
      "-rwxrwxrwx 1 root root 201K Oct 22 16:36 test.tsv\n",
      "-rwxrwxrwx 1 root root 135K Oct 22 16:38 text_dev.txt\n",
      "-rwxrwxrwx 1 root root 138K Oct 22 16:38 text_test.txt\n",
      "-rwxrwxrwx 1 root root 760K Oct 22 16:38 text_train.txt\n",
      "-rwxrwxrwx 1 root root 1.1M Oct 22 16:36 train.tsv\n"
     ]
    }
   ],
   "source": [
    "NER_DATA_DIR = 'DATA_DIR/NER'\n",
    "!ls -lh $NER_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert these to a format that is compatible with [NeMo Token Classification module](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/token_classification.py), using the [conversion script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/data/import_from_iob_format.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER task requires two files: the text sentences, and the labels.  Run the next two cells to see a sample of the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor . \n",
      "The adenomatous polyposis coli ( APC ) tumour - suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK - 3beta ) , axin / conductin and betacatenin . \n",
      "Complex formation induces the rapid degradation of betacatenin . \n",
      "In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf - 4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . \n",
      "Here , we report the identification and genomic structure of APC homologues . \n",
      "Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . \n",
      "Like APC , APC2 regulates the formation of active betacatenin - Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . \n",
      "Human APC2 maps to chromosome 19p13 . \n",
      "3 . \n",
      "APC and APC2 may therefore have comparable functions in development and cancer . \n"
     ]
    }
   ],
   "source": [
    "!head $NER_DATA_DIR/text_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O B-Disease I-Disease I-Disease I-Disease O O \n",
      "O B-Disease I-Disease I-Disease I-Disease I-Disease I-Disease I-Disease O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O \n",
      "O B-Disease I-Disease O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O B-Disease I-Disease O O \n",
      "O O O O O O O \n",
      "O O \n",
      "O O O O O O O O O O O B-Disease O \n"
     ]
    }
   ],
   "source": [
    "!head $NER_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB Tagging\n",
    "We can see that the abstract has been broken into sentences.  Each sentence is then further parsed into words with labels that correspond to the original HTML-style tags in the corpus. \n",
    "\n",
    "The sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging. Anything separated by white space is a word, including punctuation.  For the first sentence we have the following mapping:\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "Recall the original corpus tags:\n",
    "```html\n",
    "Identification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor .\n",
    "```\n",
    "The beginning word of the tagged text, \"adenomatous\", is now IOB-tagged with a <span style=\"font-family:verdana;font-size:110%;\">B</span> (beginning) tag, the other parts of the disease, \"polyposis coli tumour\" tagged with <span style=\"font-family:verdana;font-size:110%;\">I</span> (inside) tags, and everything else tagged as <span style=\"font-family:verdana;font-size:110%;\">O</span> (outside)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "\n",
    "Our Named Entity Recognition model is comprised of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a Token Classification layer.\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "MODEL_CONFIG = \"token_classification_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file...\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/token_classification_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
    "config.model.train_ds.batch_size=12#16#12#8\n",
    "config.model.validation_ds.batch_size=12#16#12#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model: null\n",
      "trainer:\n",
      "  gpus: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 5\n",
      "  max_steps: null\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  amp_level: O0\n",
      "  precision: 16\n",
      "  accelerator: ddp\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "  resume_from_checkpoint: null\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: token_classification_model\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "model:\n",
      "  nemo_path: null\n",
      "  label_ids: null\n",
      "  dataset:\n",
      "    data_dir: ???\n",
      "    class_balancing: null\n",
      "    max_seq_length: 128\n",
      "    pad_label: O\n",
      "    ignore_extra_tokens: false\n",
      "    ignore_start_end: false\n",
      "    use_cache: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 12\n",
      "  validation_ds:\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 12\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  head:\n",
      "    num_fc_layers: 2\n",
      "    fc_dropout: 0.5\n",
      "    activation: relu\n",
      "    log_softmax: true\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 5.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Setting up Data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. \n",
    "So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n",
    "\n",
    "Also notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user.\n",
    "\n",
    "Let's now add the data directory path to the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this tutorial train and dev datasets are located in the same folder, so it is enought to add the path of the data directory to the config\n",
    "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
    "\n",
    "# if you want to decrease the size of your datasets, uncomment the lines below:\n",
    "# NUM_SAMPLES = 1000\n",
    "# config.model.train_ds.num_samples = NUM_SAMPLES\n",
    "# config.model.validation_ds.num_samples = NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 16\n",
      "accelerator: ddp\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO - GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO - TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "\n",
    "# config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "config.trainer.precision = 32\n",
    "\n",
    "config.model.dataset.num_workers=4 \n",
    "config.model.optim.lr=4.232e-04\n",
    "\n",
    "# for mixed precision training(requires APEX), uncomment the line below (precision should be set to 16 and amp_level to O1):\n",
    "# config.trainer.amp_level=O1\n",
    "\n",
    "# remove distributed training flags\n",
    "# config.trainer.distributed_backend=ddp \n",
    "# config.trainer.distributed_backend=None\n",
    "# config.trainer.distributed_backend='dp'\n",
    "\n",
    "# setup max number of steps to reduce training time for demonstration purposes of this tutorial\n",
    "# config.trainer.max_steps=500\n",
    "\n",
    "# setup max number of epochs \n",
    "config.trainer.max_epochs=1#1\n",
    "\n",
    "# does not save checkpoints (faster training iterations without saves) \n",
    "config.exp_manager.create_checkpoint_callback=False\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a NeMo Experiment¶\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:29 exp_manager:169] Experiments will be logged at /mnt/batch/tasks/shared/LS_root/mounts/clusters/vac20amlofgpuv100-tr11/code/Users/Johnathon.Stringer/AzureML-NeMo/nemo_experiments/token_classification_model/2020-10-22_17-12-29\n",
      "[NeMo I 2020-10-22 17:12:30 exp_manager:503] TensorboardLogger has been set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/vac20amlofgpuv100-tr11/code/Users/Johnathon.Stringer/AzureML-NeMo/nemo_experiments/token_classification_model/2020-10-22_17-12-29'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['megatron-bert-345m-uncased', 'megatron-bert-345m-cased', 'megatron-bert-uncased', 'megatron-bert-cased', 'biomegatron-bert-345m-uncased', 'biomegatron-bert-345m-cased', 'bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'cl-tohoku/bert-base-japanese', 'cl-tohoku/bert-base-japanese-whole-word-masking', 'cl-tohoku/bert-base-japanese-char', 'cl-tohoku/bert-base-japanese-char-whole-word-masking', 'TurkuNLP/bert-base-finnish-cased-v1', 'TurkuNLP/bert-base-finnish-uncased-v1', 'wietsedv/bert-base-dutch-cased', 'distilbert-base-uncased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-cased-distilled-squad', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-finetuned-sst-2-english', 'roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector', 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1', 'albert-xxlarge-v1', 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2']\n"
     ]
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "print(nemo_nlp.modules.get_pretrained_lm_models_list())\n",
    "\n",
    "# specify BERT-like model, you want to use\n",
    "PRETRAINED_BERT_MODEL = \"biomegatron-bert-345m-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation.\n",
    "Also, the pretrained BERT model will be downloaded, note it can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:31 token_classification_model:97] Setting model.dataset.data_dir to DATA_DIR/NER.\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:70] Labels: {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:75] Labels mapping saved to : DATA_DIR/NER/label_ids.csv\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:78] Three most popular labels in train dataset:\n",
      "[NeMo I 2020-10-22 17:12:31 data_preprocessing:131] label: 0, 124819 out of 136086 (91.72%).\n",
      "[NeMo I 2020-10-22 17:12:31 data_preprocessing:131] label: 2, 6122 out of 136086 (4.50%).\n",
      "[NeMo I 2020-10-22 17:12:31 data_preprocessing:131] label: 1, 5145 out of 136086 (3.78%).\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:83] Total labels: 136086\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:84] Label frequencies - {0: 124819, 2: 6122, 1: 5145}\n",
      "[NeMo I 2020-10-22 17:12:31 token_classification_descriptor:88] Class Weights: {0: 0.36342223539685464, 2: 7.409670042469781, 1: 8.816715257531584}\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:78] Three most popular labels in test dataset:\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 0, 22450 out of 24497 (91.64%).\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 2, 1087 out of 24497 (4.44%).\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 1, 960 out of 24497 (3.92%).\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:83] Total labels: 24497\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:84] Label frequencies - {0: 22450, 2: 1087, 1: 960}\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:78] Three most popular labels in dev dataset:\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 0, 22092 out of 23969 (92.17%).\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 2, 1090 out of 23969 (4.55%).\n",
      "[NeMo I 2020-10-22 17:12:32 data_preprocessing:131] label: 1, 787 out of 23969 (3.28%).\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:83] Total labels: 23969\n",
      "[NeMo I 2020-10-22 17:12:32 token_classification_descriptor:84] Label frequencies - {0: 22092, 2: 1090, 1: 787}\n",
      "[NeMo I 2020-10-22 17:12:39 token_classification_dataset:116] Setting Max Seq length to: 128\n",
      "[NeMo I 2020-10-22 17:12:39 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2020-10-22 17:12:39 data_preprocessing:301] Min: 4 |                  Max: 178 |                  Mean: 35.974963181148745 |                  Median: 34.0\n",
      "[NeMo I 2020-10-22 17:12:39 data_preprocessing:303] 75 percentile: 45.00\n",
      "[NeMo I 2020-10-22 17:12:39 data_preprocessing:304] 99 percentile: 89.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-22 17:12:40 token_classification_dataset:145] 8 are longer than 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:150] subtokens: [CLS] I ##dent ##ification of AP ##C ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . [SEP]\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:40 token_classification_dataset:264] features saved to DATA_DIR/NER/cached_text_train.txt_BertTokenizer_128_28996_-1\n",
      "[NeMo I 2020-10-22 17:12:41 token_classification_dataset:116] Setting Max Seq length to: 122\n",
      "[NeMo I 2020-10-22 17:12:41 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2020-10-22 17:12:41 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2020-10-22 17:12:41 data_preprocessing:303] 75 percentile: 47.00\n",
      "[NeMo I 2020-10-22 17:12:41 data_preprocessing:304] 99 percentile: 83.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-22 17:12:42 token_classification_dataset:145] 0 are longer than 122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:12:42 token_classification_dataset:264] features saved to DATA_DIR/NER/cached_text_dev.txt_BertTokenizer_128_28996_-1\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... None\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  data_impl ....................... infer\n",
      "  data_path ....................... None\n",
      "  DDP_impl ........................ local\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 100\n",
      "  exit_interval ................... None\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 1024\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... True\n",
      "  load ............................ None\n",
      "  local_rank ...................... None\n",
      "  log_interval .................... 100\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. None\n",
      "  lr_decay_iters .................. None\n",
      "  lr_decay_style .................. linear\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 512\n",
      "  merge_file ...................... None\n",
      "  min_lr .......................... 0.0\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 24\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... True\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ None\n",
      "  save_interval ................... None\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... None\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 969, 30, 1\n",
      "  tensorboard_dir ................. None\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. BertWordPieceLowerCase\n",
      "  train_iters ..................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  vocab_file ...................... /home/azureuser/.cache/torch/megatron/biomegatron-bert-345m-cased_vocab\n",
      "  warmup .......................... 0.01\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "---------------- end of arguments ----------------\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2020-10-22 17:12:42 megatron_bert:92] Megatron-lm argparse args: Namespace(DDP_impl='local', adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=False, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, batch_size=None, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, data_impl='infer', data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', dynamic_loss_scale=True, eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_style='linear', make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, min_lr=0.0, min_scale=1, mmap_warmup=False, model_parallel_size=1, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_unique_layers=None, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, param_sharing_style='grouped', params_dtype=torch.float32, query_in_block_prob=0.1, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/home/azureuser/.cache/torch/megatron/biomegatron-bert-345m-cased_vocab', warmup=0.01, weight_decay=0.01, world_size=1)\n",
      "[NeMo I 2020-10-22 17:12:46 megatron_bert:138] restore_path: /home/azureuser/.cache/torch/megatron/biomegatron-bert-345m-cased is a file. Assuming no megatron model parallelism\n",
      "[NeMo I 2020-10-22 17:12:49 megatron_bert:145] weights restored from /home/azureuser/.cache/torch/megatron/biomegatron-bert-345m-cased\n"
     ]
    }
   ],
   "source": [
    "model_ner = nemo_nlp.models.TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress.\n",
    "If you're not using Colab, refer to [https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) if you're facing issues with running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:50 modelPT:583] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 0.0004232\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2020-10-22 17:12:50 lr_scheduler:554] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fda6c8cc748>\" \n",
      "    will be used during training (effective maximum steps = 453) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 453\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertEncoder  | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1 M   \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "INFO - \n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertEncoder  | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1 M   \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-22 17:12:50 nemo_logging:349] /anaconda/envs/azureml_py36/lib/python3.6/site-packages/nemo/collections/nlp/metrics/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:\n",
      "    \tnonzero(Tensor input, *, Tensor out)\n",
      "    Consider using one of the following signatures instead:\n",
      "    \tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:12:51 token_classification_model:173] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         93.47      84.89      88.97        556\n",
      "    B-Disease (label_id: 1)                                  0.00       0.00       0.00         17\n",
      "    I-Disease (label_id: 2)                                  2.27      10.00       3.70         20\n",
      "    -------------------\n",
      "    micro avg                                               79.93      79.93      79.93        593\n",
      "    macro avg                                               31.91      31.63      30.89        593\n",
      "    weighted avg                                            87.71      79.93      83.55        593\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c07553ae4d494fbdfbd64c86cce933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:15:55 token_classification_model:173] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.35      98.91      98.12      22092\n",
      "    B-Disease (label_id: 1)                                 38.02      31.26      34.31        787\n",
      "    I-Disease (label_id: 2)                                 60.62      48.72      54.02       1090\n",
      "    -------------------\n",
      "    micro avg                                               94.41      94.41      94.41      23969\n",
      "    macro avg                                               65.33      59.63      62.15      23969\n",
      "    weighted avg                                            93.73      94.41      94.02      23969\n",
      "    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(model_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To see how the model performs, we can run generate prediction similar to the way we did it earlier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# let's first create a subset of our dev data\n",
    "! head -n 100 {DATA_DIR}/NER/text_dev.txt > {DATA_DIR}/NER/sample_text_dev.txt\n",
    "! head -n 100 {DATA_DIR}/NER/labels_dev.txt > {DATA_DIR}/NER/sample_labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate predictions for the provided text file.\n",
    "If labels file is also specified, the model will evaluate the predictions and plot confusion matrix. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_ner.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'NER', 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'NER', 'sample_labels_dev.txt'),\n",
    "    output_dir=exp_dir,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")\n",
    "# Please check matplotlib version if encountering any error plotting confusion matrix:\n",
    "# https://stackoverflow.com/questions/63212347/importerror-cannot-import-name-png-from-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:116] Setting Max Seq length to: 122\n",
      "[NeMo I 2020-10-22 17:15:58 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2020-10-22 17:15:58 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2020-10-22 17:15:58 data_preprocessing:303] 75 percentile: 47.00\n",
      "[NeMo I 2020-10-22 17:15:58 data_preprocessing:304] 99 percentile: 83.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-22 17:15:58 token_classification_dataset:145] 0 are longer than 122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:15:58 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2020-10-22 17:16:31 token_classification_model:411] Labels save to /mnt/batch/tasks/shared/LS_root/mounts/clusters/vac20amlofgpuv100-tr11/code/Users/Johnathon.Stringer/AzureML-NeMo/nemo_experiments/token_classification_model/2020-10-22_17-12-29/infer_text_dev.txt\n",
      "[NeMo I 2020-10-22 17:16:31 token_classification_model:417] Predictions saved to /mnt/batch/tasks/shared/LS_root/mounts/clusters/vac20amlofgpuv100-tr11/code/Users/Johnathon.Stringer/AzureML-NeMo/nemo_experiments/token_classification_model/2020-10-22_17-12-29/infer_text_dev.txt\n",
      "23969 23969\n",
      "[NeMo I 2020-10-22 17:16:32 utils_funcs:94] Confusion matrix saved to /mnt/batch/tasks/shared/LS_root/mounts/clusters/vac20amlofgpuv100-tr11/code/Users/Johnathon.Stringer/AzureML-NeMo/nemo_experiments/token_classification_model/2020-10-22_17-12-29/Normalized_Confusion_matrix_20201022-171632\n",
      "[NeMo I 2020-10-22 17:16:32 token_classification_model:428]                          precision    recall  f1-score   support\n",
      "    \n",
      "            O (label id: 0)     0.9735    0.9891    0.9812     22092\n",
      "    B-Disease (label id: 1)     0.3802    0.3126    0.3431       787\n",
      "    I-Disease (label id: 2)     0.6062    0.4872    0.5402      1090\n",
      "    \n",
      "                   accuracy                         0.9441     23969\n",
      "                  macro avg     0.6533    0.5963    0.6215     23969\n",
      "               weighted avg     0.9373    0.9441    0.9402     23969\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ner.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'NER', 'text_dev.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'NER', 'labels_dev.txt'),\n",
    "    output_dir=exp_dir,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")\n",
    "# Please check matplotlib version if encountering any error plotting confusion matrix:\n",
    "# https://stackoverflow.com/questions/63212347/importerror-cannot-import-name-png-from-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "If you have NeMo installed locally, you can also train the model with `nlp/token_classification/token_classification.py.`\n",
    "\n",
    "To run training script, use:\n",
    "\n",
    "`python token_classification.py model.dataset.data_dir=PATH_TO_DATA_DIR PRETRAINED_BERT_MODEL=biomegatron-bert-345m-cased`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training could take several minutes and the result should look something like\n",
    "```\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:82] Accuracy: 0.9882348032875798\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 weighted: 98.82\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 macro: 93.74\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 micro: 98.82\n",
    "[NeMo I 2020-05-22 17:13:49 token_classification_callback:89] precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9938    0.9957    0.9947     22092\n",
    "    B (label id: 1)     0.8843    0.9034    0.8938       787\n",
    "    I (label id: 2)     0.9505    0.8982    0.9236      1090\n",
    "    \n",
    "           accuracy                         0.9882     23969\n",
    "          macro avg     0.9429    0.9324    0.9374     23969\n",
    "       weighted avg     0.9882    0.9882    0.9882     23969\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "or you can use bash launch-nemo-ner.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
